---
layout: single
title:  "Generative Radiance Fields for 3D-Aware Image Synthesis"
date:   2020-11-30 07:15 +0200
categories: "paper"
tags: ["deep learning", "generative adversarial networks", "unsupervised", "3d controllability", "3d awareness", "radiance fields"]
author: "Katja Schwarz"
excerpt: >-
    Generative Radiance Fields generate 3D-consistent images, scale well to high resolution and require only unposed 2D images for training.
header:
    teaser: "/assets/posts/2020-11-30-graf/teaser.gif"
---

Generative adversarial networks have enabled photorealistic and high-resolution image synthesis. 
But they have one limitation: 
Say we want to rotate the camera viewpoint for the cars below, then generic GANs cannot model this explicitly.

![generic_gans]({{ site.url }}/assets/posts/2020-11-30-graf/generic_gans.gif){: .align-center}

To truly leverage the power of generative models for content creation, e.g. for virtual reality or simulation,
it is important to take the 3D nature of our world into account. 
Therefore, our goal is to make generative models **3D-aware**. 
Importantly, we want to achieve this without using 3D supervision, e.g. 3D shapes or posed multi-view images, 
because this information can be difficult to obtain for real world data.
In a nutshell, the research question we address in this project is: 

*How can we create a 3D-aware generative model that can learn from unposed 2D images only?*


## How to design a 3D-aware GAN

Generic generative models create images from a low-dimensional latent code that represents properties like shape or appearance of the object.
![generic_pipeline]({{ site.url }}/assets/posts/2020-11-30-graf/generic_GAN.png){: .align-center}

We can incorporate 3D-awareness by adding a virtual camera to the model. 
This introduces two additional components:
1. A 3D representation of the generated objects, parameterized by a 3D Generator 
2. A virtual camera and a corresponding renderer that produces an image of the 3D representation

![3d_pipeline]({{ site.url }}/assets/posts/2020-11-30-graf/3d_GAN.png){: .align-center}

The key contribution of GRAF lies in a clever choice of these components.
Previous works (
[Nguyen-Phuoc et al., 2019](https://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Nguyen-Phuoc_HoloGAN_Unsupervised_Learning_of_3D_Representations_From_Natural_Images_ICCVW_2019_paper.pdf),
[Henzler et al., 2019](https://openaccess.thecvf.com/content_ICCV_2019/papers/Henzler_Escaping_Platos_Cave_3D_Shape_From_Adversarial_Rendering_ICCV_2019_paper.pdf)
) use voxel-based 3D-representations. The problem with voxels is that they scale cubically in memory with increasing size. 
Inspired by [NeRF](https://arxiv.org/pdf/2003.08934.pdf), we instead represent the 3D-object with a **continuous** function, namely a **G**enerative **RA**diance **F**ield (GRAF).
As evidenced by our experiments, this allows to generate 3D-consistent images, scales better to high resolution and requires only unposed 2D images for training.


## Spin it around!

With GRAF, we can now render images from different viewpoints by controlling the pose of our virtual camera in the model.

![3d_gans]({{ site.url }}/assets/posts/2020-11-30-graf/3d_gans.gif){: .align-center}

And of course our method works not only on synthetic cars! For example, we can train the network using chair images:

![chairs]({{ site.url }}/assets/posts/2020-11-30-graf/chairs.gif){: .align-center}

Remember, that the network only saw unposed 2D images during training which makes generating 3D objects a difficult endeavor.
Nonetheless, GRAF also works reasonably well for natural images. 

![cats]({{ site.url }}/assets/posts/2020-11-30-graf/cats.gif){: .align-center}
![birds]({{ site.url }}/assets/posts/2020-11-30-graf/birds.gif){: .align-center}
![faces]({{ site.url }}/assets/posts/2020-11-30-graf/faces.gif){: .align-center}


## Go into depth!

One benefit of generative radiance fields is that they additionally produce a depth map to every RGB image.
![carla_depth]({{ site.url }}/assets/posts/2020-11-30-graf/carla_depth.gif){: .align-center}

![faces_depth]({{ site.url }}/assets/posts/2020-11-30-graf/faces_depth.gif){: .align-center}

## Play around!

Moreover, instead of using a single latent code to model the entire image like generic GANs, 
GRAF models shape and appearance separately using two disentangled latent codes and allows for modifying them separately.
As a result, we can play around with shape and appearance of the generated objects, respectively.

Modifying only shape:
![carla_shape]({{ site.url }}/assets/posts/2020-11-30-graf/carla_shape.gif){: .align-center}

Modifying only appearance:
![carla_appearance]({{ site.url }}/assets/posts/2020-11-30-graf/carla_appearance.gif){: .align-center}


## Curious?

For more details, including a nice visualization of generative radiance fields, watch our 3-minute video:

{% include video id="akQf7WaCOHo" provider="youtube" %}

Further information (including the [paper](http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf) and [supplementary](http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf)) is available on our [project page](https://avg.is.tuebingen.mpg.de/publications/schwarz2020neurips).
If you are interested in experimenting with GRAF yourself, download the [source code](https://github.com/autonomousvision/graf) of our project and run the examples. 
We are happy to receive your feedback!

    @inproceedings{Schwarz2020NEURIPS,
      title = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
      author = {Schwarz, Katja and Liao, Yiyi and Niemeyer, Michael and Geiger, Andreas},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
      year = {2020}
    }
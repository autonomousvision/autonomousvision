---
layout: single
title:  "DeFuSR: Learning Non-volumetric Depth Fusion using Successive Reprojections"
date:   2019-06-26 12:30 +0200
categories: "paper"
tags: ["3D reconstruction", "deep learning", "depth fusion", "multi-view stereo"]
author: "Simon Donne"
excerpt: >-
    We propose to combine the prior work on multi-view geometry and triangulation with the strength of deep neural networks. To this end, we combine a learning-based depth refinement/fusion step with well established multi-view stereo techniques (both traditional and learning-based).
header:
    teaser: "/assets/posts/2019-06-26-defusr/teaser.png"
---

# DeFuSR: Learning Non-volumetric Depth Fusion using Successive Reprojections

In pretty much every field of data processing, computer vision included, deep learning is throwing top approaches from their throne.
In computer vision, the current wave of deep learning has started mostly in image classification.
Beginning with relatively easy recognition problems (does this image show a dog or a cat?), networks quickly became better at classifying subspecies than most humans.
Whereas hand-crafted techniques for classification can only leverage the knowledge of their creators, neural networks distill decision rules directly from data; perfect in the case of classification, where the optimal decision rules can be incredibly complex and are typically not known in advance.

It is likely for this reason that the problem of multi-view stereo has only very recently seen neural networks outperform existing techniques.
With multi-view stereo, we estimate the 3D structure of a scene based on multiple images of that scene.
In the most simple case, the locations $\tilde{u}_L$ and $\tilde{u}_R$ at which two different cameras observe object $x$ can be used to triangulate that object's location (see the figure below).
Here, researchers have always had a clear idea on how to go from images (and their correspondences) to 3D location, and so deep learning has needed some time to get traction in the field.
With [Learning Non-volumetric Depth Fusion using Successive Reprojections](http://www.cvlibs.net/publications/Donne2019CVPR.pdf), we propose to combine the prior knowledge about multi-view geometry and triangulation with the strength of deep neural networks.  

![Triangulation for finding the location of an object]({{ site.url }}/assets/posts/2019-06-26-defusr/twoview_triangulation.png){: .align-center width="500px"}

## Combining traditional and learning-based approaches for Multi-view Stereo

Before being able to triangulate object $x$ based on the different images, we of course need to have the locations $\tilde{u}_i$ in the images: the correspondence problem.
The multi-view stereo pipeline is an established approach to this problem:

1. **Feature description**  
We describe the different possible positions in the images (often simply all pixels) with a feature vector.
The implication/assumption is that, if the feature description of two image locations $\tilde{u}_1$ and $\tilde{u}_2$ match, that they belong to the same 3D object and can be used for triangulation.

2. **Cost volume calculation**  
For a set of 3D positions, we compare the corresponding feature descriptions in all views.
If these are similar, then the cost is low: there is likely to be an object here (because otherwise, the feature descriptions are unlikely to match).
Often, the set of 3D positions is chosen based on a center view: in this case, we draw a line through each point of interest, and compare with all the other descriptions -- a technique called planesweeping.

![Planesweeping to decide on point x]({{ site.url }}/assets/posts/2019-06-26-defusr/planesweeping.png){: .align-center width="700px"}

3. **Depth estimation**  
From this cost volume we then decide on the most likely location along the line to contain an object.
Often, the resulting estimates are filtered: depth is smooth nearly everywhere but with a few very sharp edges at object boundaries.
By leveraging this we can, e.g., filter out bad estimates, or even improve on the existing ones.

4. **Depth fusion**  
Finally, we wish to arrive at a 3D representation of the scene.
The easiest way to do this is to simply take all the points $x$ found by the planesweeping.
However, even after filtering the estimates, this is likely to be noisy.
Traditional techniques require that points are supported by multiple views, i.e. for each point $x$ in the combined set, several images need to have a similar point close-by in order for it to be in the final reconstruction.

Various parts of this pipeline are slowly being replaced by learned alternatives.
The first was the feature description step.
Neural networks are extraordinarily good at generating distinctive features for subsequent matching, as shown by [Hartmann et al.](https://www.ethz.ch/content/dam/ethz/special-interest/baug/igp/photogrammetry-remote-sensing-dam/documents/pdf/Papers/Learned-Multi-Patch-Similarity.pdf) and [MVSNet](https://eccv2018.org/openaccess/content_ECCV_2018/papers/Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper.pdf), among others.
Depth filtering of a single image was succesfully shown by, e.g., [DDRNet](http://www.liuyebin.com/DDRNet/DDRNet.pdf).
Some approaches, such as [SurfaceNet](http://openaccess.thecvf.com/content_ICCV_2017/papers/Ji_SurfaceNet_An_End-To-End_ICCV_2017_paper.pdf) replaced the entire pipeline with a single neural network, but they are somewhat less flexible and general than the tried-and-true pipeline.
The strong geometrical knowledge encoded by the planesweeping proved to be of utmost importance.

The final step, combining the different depth maps into a single scene representation, was also tackled from a learning-based angle.
However, it is not immediately obvious what the best representation is.
While point clouds or meshes offer a compact representation, they are hard for neural networks to work with.
The structured approach of a voxel grid is much better, but requires much more memory.
This was discussed in more detail in our blog post on [occupancy networks](https://autonomousvision.github.io/occupancy-networks/), where we offer an alternative.
For this work, we chose the depth maps themselves: not quite as rich as volumetric grids, but much more memory efficient.

## Our fusion approach

We work with each view as the center view in turn, directly after performing planesweeping for all views.
To leverage the information of neighbouring views, we reproject the information from all neighbours onto the center view.
These reprojections are then used to refine the initial depth estimate from planesweeping.

![Refining the center view based on neighbours]({{ site.url }}/assets/posts/2019-06-26-defusr/teaser.png){: .align-center width="700px"}

Interesting here is the way we perform this reprojection.
As shown in the image below, there are two types of information we get from a neighbouring view.
First of all, in light blue, the surfaces that neighbour observes and that are expected to be close to the surface the center view observes.
However, because of the sharp depth edges in the depth maps, we also have a secondary source of information: in orange, the implied depth bounds.
The center view knows that objects in those parts of its image should not be closer than the orange line, because otherwise they would not be compatible with the neighbour's depth estimate. 

![The bounds implied by a neighbouring view]({{ site.url }}/assets/posts/2019-06-26-defusr/impliedbounds.png){: .align-center width="500px"}


The original depth estimate as well as the input image, and all the reprojected neighbour information, is fed into a neural network that returns both an improved depth estimate and a confidence estimate for this new depth.

We perform multiple rounds of this depth fusion: as after each iteration, all depth maps improve, the neighbour information in subsequent iterations is of higher quality.
In practice, the performance quickly saturates and performing three such iterations suffices.
Here, we show the resulting depth error in terms of the iterations, for depth maps estimated by two different techniques ([COLMAP](https://demuc.de/colmap/) and [MVSNet](https://eccv2018.org/openaccess/content_ECCV_2018/papers/Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper.pdf)).
Dark blue represents zero error, while dark red represents infinite error.
Specifically in MVSNet's initial result we see heavy quantization because its cost volume is of relatively small resolution; a larger one does not fit into memory for the training procedure.  

![The impact of iterations on the reconstruction quality]({{ site.url }}/assets/posts/2019-06-26-defusr/iterations.png){: .align-center}

Finally, we have also obtained very nice results using the trained network on real data, which was notably different from the data we have trained the network on (a different camera, different object types and changed environments).
Despite these differences, the network performed admirably in refining and completing the depth for these test cases.
This generalization is likely because, as far as our approach is concerned, the depth is of the same type as was trained on (as coming from [COLMAP](https://demuc.de/colmap/)).  

![Evaluation our approach on data it wasn't trained on]({{ site.url }}/assets/posts/2019-06-26-defusr/realdata.png){: .align-center width="600px"}

## Further Information

You can find more information (including the paper and supplementary) on [our project page](https://avg.is.tuebingen.mpg.de/research_projects/defusr).
The teaser video is available [on YouTube](http://www.youtube.com/watch?v=Cz7zz7Fuqlg&vq=hd1080&autoplay=1).
If you are interested in experimenting with our occupancy networks yourself, download the source code of our project and run the examples.
We are happy to receive your feedback!

    @inproceedings{DeFuSR,
        title = {DeFuSR: Learning Non-volumetric Depth Fusion using Successive Reprojections},
        author = {Donne, Simon and Geiger, Andreas},
        booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
        year = {2019}
    }

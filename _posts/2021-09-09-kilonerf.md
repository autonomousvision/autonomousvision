---
layout: single
title:  "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs"
date:   2021-09-09 07:15 +0100
categories: "paper"
tags: ["novel view synthesis", "real-time rendering", "implicit representations", "neural radiance fields"]
author: "Christian Reiser"
excerpt: >-
    TODO
header:
    teaser: "/assets/posts/2020-11-30-graf/teaser.gif"
---

In novel view synthesis (NVS), we are given a number of 2D images of a scene and we want to give the user the ability to look at the scene from abitrary unobserved viewpoints. Recently, with Neural Radiance Fields (NeRF) a NVS technique appeared that receives a lot of attention from the research community thanks to its simplicity, extensibility and the unprecedented quality of the novel views NeRF synthesizes. NeRF also inspired countless follow-up works. This can be mainly attributed to the representation used by NeRF. NeRF represents the scene as a Multi-Layer Perceptron (MLP). This neural network gets as input a 3D position and a viewing direction and gives out volumetric density and RGB color values for the given combination of position and viewing direction. In contrast to an explicit representation like a voxel grid no resolution has to be specified and the resulting representation is highly compressed. In fact, storing medium-size scenes in high detail is possible with about 5 megabyte whereas a voxel grid of sufficiently high resolution consumes several gigabytes of disk storage. The issue with this MLP representation is that syntheszing novel views is painfully slow. For each point that we want to query this representation a forward pass through the MLP is required. This means that we have to execute a sequence of expensive vector-matrix multiplications to decode each individual point. Naturally, decoding would be faster if we could get away with using a smaller MLP, i.e. one with a smaller number of neurons and/or hidden layers. However, if we just use a smaller MLP without any other measures the reduced MLP's capacity is not sufficient anymore to faithfully represent the whole scene. Our simple idea is to first subdivide the scene into a coarse voxel grid and assign a seperate MLP to each voxel cell. Since each of these MLPs only has to represent a small part of the scene tiny MLPs suffice to represent the scene with high detail.


![generic_gans]({{ site.url }}/assets/posts/2020-11-30-graf/generic_gans.gif){: .align-center}

To truly leverage the power of generative models for content creation, e.g. for virtual reality or simulation,
it is important to take the 3D nature of our world into account. 
Therefore, our goal is to make generative models **3D-aware**. 
Importantly, we want to achieve this without using 3D supervision, e.g. 3D shapes or posed multi-view images, 
because this information can be difficult to obtain for real world data.
In a nutshell, the research question we address in this project is: 

*How can we create a 3D-aware generative model that can learn from unposed 2D images only?*


## How to design a 3D-aware GAN

Generic generative models create images from a low-dimensional latent code that represents properties like shape or appearance of the object.
![generic_pipeline]({{ site.url }}/assets/posts/2020-11-30-graf/generic_GAN.png){: .align-center}

We can incorporate 3D-awareness by adding a virtual camera to the model. 
This introduces two additional components:
1. A 3D representation of the generated objects, parameterized by a 3D Generator 
2. A virtual camera and a corresponding renderer that produces an image of the 3D representation

![3d_pipeline]({{ site.url }}/assets/posts/2020-11-30-graf/3d_GAN.png){: .align-center}

The key contribution of GRAF lies in a clever choice of these components.
Previous works (
[Nguyen-Phuoc et al., 2019](https://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Nguyen-Phuoc_HoloGAN_Unsupervised_Learning_of_3D_Representations_From_Natural_Images_ICCVW_2019_paper.pdf),
[Henzler et al., 2019](https://openaccess.thecvf.com/content_ICCV_2019/papers/Henzler_Escaping_Platos_Cave_3D_Shape_From_Adversarial_Rendering_ICCV_2019_paper.pdf)
) use voxel-based 3D-representations. The problem with voxels is that they scale cubically in memory with increasing size. 
Inspired by [NeRF](https://arxiv.org/pdf/2003.08934.pdf), we instead represent the 3D-object with a **continuous** function, namely a **G**enerative **RA**diance **F**ield (GRAF).
As evidenced by our experiments, this allows to generate 3D-consistent images, scales better to high resolution and requires only unposed 2D images for training.


## Spin it around!

With GRAF, we can now render images from different viewpoints by controlling the pose of our virtual camera in the model.

![3d_gans]({{ site.url }}/assets/posts/2020-11-30-graf/3d_gans.gif){: .align-center}

And of course our method works not only on synthetic cars! For example, we can train the network using chair images:

![chairs]({{ site.url }}/assets/posts/2020-11-30-graf/chairs.gif){: .align-center}

Remember, that the network only saw unposed 2D images during training which makes generating 3D objects a difficult endeavor.
Nonetheless, GRAF also works reasonably well for natural images. 

![cats]({{ site.url }}/assets/posts/2020-11-30-graf/cats.gif){: .align-center}
![birds]({{ site.url }}/assets/posts/2020-11-30-graf/birds.gif){: .align-center}
![faces]({{ site.url }}/assets/posts/2020-11-30-graf/faces.gif){: .align-center}


## Go into depth!

One benefit of generative radiance fields is that they additionally produce a depth map to every RGB image.
![carla_depth]({{ site.url }}/assets/posts/2020-11-30-graf/carla_depth.gif){: .align-center}

![faces_depth]({{ site.url }}/assets/posts/2020-11-30-graf/faces_depth.gif){: .align-center}

## Play around!

Moreover, instead of using a single latent code to model the entire image like generic GANs, 
GRAF models shape and appearance separately using two disentangled latent codes and allows for modifying them separately.
As a result, we can play around with shape and appearance of the generated objects, respectively.

Modifying only shape:
![carla_shape]({{ site.url }}/assets/posts/2020-11-30-graf/carla_shape.gif){: .align-center}

Modifying only appearance:
![carla_appearance]({{ site.url }}/assets/posts/2020-11-30-graf/carla_appearance.gif){: .align-center}


## Curious?

For more details, including a nice visualization of generative radiance fields, watch our 3-minute video:

{% include video id="akQf7WaCOHo" provider="youtube" %}

Further information (including the [paper](http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf) and [supplementary](http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf)) is available on our [project page](https://avg.is.tuebingen.mpg.de/publications/schwarz2020neurips).
If you are interested in experimenting with GRAF yourself, download the [source code](https://github.com/autonomousvision/graf) of our project and run the examples. 
We are happy to receive your feedback!

    @inproceedings{Schwarz2020NEURIPS,
      title = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
      author = {Schwarz, Katja and Liao, Yiyi and Niemeyer, Michael and Geiger, Andreas},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
      year = {2020}
    }
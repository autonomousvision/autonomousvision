---
layout: single
title:  "Counterfactual Generative Networks"
date:   2021-02-09 10:00 +0200
categories: "paper"
tags: ["causality", "counterfactuals", "generative models", "robustness", "image classification", "data augmentation"]
author: "Axel Sauer"
excerpt: >-
    A generative model structured into independent causal mechanisms produces images for training invariant classifiers.
header:
    overlay_image: "/assets/posts/2021-02-09-cgn/teaser2.png"
    overlay_filter: 0.4
    teaser: "/assets/posts/2021-02-09-cgn/teaser.gif"
---

What do you see in the following images?

![cows]({{ site.url }}/assets/posts/2021-02-09-cgn/cows.png){: .align-center}

If you answered Camel, Cow, Camel, and Cow, well done! In many instances, machine learning models are not that smart yet. If we collect a dataset for this Camel-Cow classification task, images in the dataset will typically depict cows on green pastures and camels in the desert. The most straightforward correlation a classifier can learn is the connection of the label “cow” to a grass-textured background and of the label “camel” to a sandy background. There are many more such examples, known as [shortcuts](https://thegradient.pub/shortcuts-neural-networks-love-to-cheat/), e.g., [Clever Hans](https://en.wikipedia.org/wiki/Clever_Hans) or the [Tank Legend](https://www.gwern.net/Tanks). Learning those shortcuts becomes problematic if the test data does not follow the training distribution - in our example, an image of a cow in the desert. In the [causality literature](https://philpapers.org/rec/PEACMR), such a correlation is called spurious.

Causality also aims to answer _counterfactual_ questions like “How would this image look like with a different background.” If we were able to answer this question, we could generate images of unseen combinations, i.e., cows in the desert, and supply them to our classifier. Training on this counterfactual data would increase robustness against spurious correlations.

## Generative Models struggle too!

So, let’s generate counterfactual images! Unfortunately, it is not that easy with standard generative models like a [VAE](https://arxiv.org/abs/1606.05908). Consider the simple example of [colored MNIST](https://arxiv.org/abs/1812.10352), a digit-classification dataset, where each digit is strongly correlated with a specific color (left). If we train a VAE on this dataset, its latent space will look like on the right:

![cows]({{ site.url }}/assets/posts/2021-02-09-cgn/colored_mnist_disentanglement.png){: .align-center}

There are nice clusters of red zeroes, yello eights, etc. But there is not a single blue zero -- the different factors of variation (FoV) in the data are not disentangled.

## Building a Counterfactual Generative Network

How do we get direct control over these factors? And how can we scale our generative model beyond toy datasets so we
generate realistic-looking images? The main idea is to structure the generative network into several subnetworks,
so-called _independent mechanisms_.  Each of them is responsible for one FoV. In the image below, there are three
mechanisms, shape $\mathbf{m}$, foreground $\mathbf{f}$, and background $\mathbf{b}$. We composite their output into the final image $\mathbf{x_{gen}}$. During training, all mechanisms receive the same label, here “ ring-tailed lemur.” We refer to the whole model as _Counterfactual Generative Network_ (CGN).

![system]({{ site.url }}/assets/posts/2021-02-09-cgn/systemv2_cropped.png){: .align-center}

By choosing appropriate losses for the optimization, we can disentangle the signal, e.g., generating a background
without any object. With the help of pre-trained models ([BigGAN](https://arxiv.org/abs/1809.11096), [U2-Net](https://arxiv.org/abs/2005.09007)), we scale the CGN to [ImageNet](http://www.image-net.org/), a large dataset that remained out of reach for previous disentanglement methods.

## Generating Counterfactuals

To generate counterfactual images, we can now sample a noise vector, generate an image, and ask: “How would this image look like if the object had a cheetah texture?” Answering this questions is as simple as supplying the cheetah label to the texture mechanisms to generate the cheetah texture. We have now full control over the FoVs for the generative process:

![teaser]({{ site.url }}/assets/posts/2021-02-09-cgn/teaser.gif){: .align-center}

The counterfactual images with randomized labels can look quite funny and intriguing:

![cfs]({{ site.url }}/assets/posts/2021-02-09-cgn/cfs.png){: .align-center}

Training on these images results in the desired effect of less reliance on backgrounds or textures. Another cool find is that a CGN does interesting things during training, like generating high-quality binary object masks and unsupervised image inpainting.

![training]({{ site.url }}/assets/posts/2021-02-09-cgn/330_all_cropped.gif){: .align-center}

From left to right: mask, foreground, background, and composite image over the course of training.

## Fun & Further Information

In GAN literature it is common to visualize the latent space with a latent walk, i.e., sampling and interpolating
between points in the latent space. Latent walks are way more interesting if we sync them to music! So, see below the
Counterfactual Latent Dance, a musical video / latent walk with a CGN.

{% include video id="JDwaLueR35U" provider="youtube" %}

Another fun application is to use the generated textures for [image stylization](https://en.wikipedia.org/wiki/Neural_Style_Transfer). We walk through the latent space of the texture mechanism and optimize the input image to follow the style of the generated texture:

![stylization]({{ site.url }}/assets/posts/2021-02-09-cgn/stylization.gif){: .align-center}

Further information (including the [paper](http://www.cvlibs.net/publications/Sauer2021ICLR.pdf) and [code](https://github.com/autonomousvision/counterfactual_generative_networks) is available on our [project page](https://sites.google.com/view/counterfactual-generation/home). If you want to generate counterfactual images directly in your browser, you can try the [Colab](https://colab.research.google.com/gist/xl-sr/ad93d3918e456fd4cd4aabdcaad87148/generating_counterfactuals.ipynb).

    @inproceedings{Sauer2021ICLR,
      title = {Counterfactual Generative Networks},
      author = {Sauer, Axel and Geiger, Andreas},
      booktitle = {International Conference on Learning Representations (ICLR)},
      year = {2021}
    }
